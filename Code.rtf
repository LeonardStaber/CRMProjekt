{\rtf1\ansi\ansicpg1252\cocoartf2761
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica-Bold;\f1\fnil\fcharset0 Menlo-Regular;\f2\fswiss\fcharset0 Helvetica;
}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\csgray\c0;}
\paperw11900\paperh16840\margl1440\margr1440\vieww13340\viewh15680\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\b\fs24 \cf0 TERMINAL\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b0\fs22 \cf2 \CocoaLigature0 pip3 install pandas\
pip3 install scikit-learn\
\pard\pardeftab720\partightenfactor0

\f2\fs24 pip3 install matplotlib\cf0 \CocoaLigature1 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
import pandas as pd\
\
from sklearn.preprocessing import StandardScaler, LabelEncoder\
\pard\pardeftab720\partightenfactor0
\cf0 from sklearn.cluster import KMeans\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
file_path = "/Users/leonardstaber/Documents/Uni/Semester 7/CRM/Projekt/fitness.csv"\
\
data = pd.read_csv("/Users/leonardstaber/Documents/Uni/Semester 7/CRM/Projekt/fitness.csv")\
\
print(data.head())\
\
print(data.isnull().sum()) #check NAs\
\
print(data.describe())\
\
# Encode categorical columns\
data['Geschlecht'] = LabelEncoder().fit_transform(data['Geschlecht'])\
data['Raucher'] = LabelEncoder().fit_transform(data['Raucher'])\
data = pd.get_dummies(data, columns=[\'91Region\'92], drop_first=True)\
\
\
features = ['Alter\'92, \'91Geschlecht\'92, \'91Kinder\'92, \'91Raucher\'92, 'BMI', \'91Ausgaben\'92] + \\\
           [col for col in data.columns if col.startswith('Region_')] # Select relevant features for clustering\
\
\
scaler = StandardScaler()\
scaled_data = scaler.fit_transform(data[features]) # Normalize data CHECK TO SEE F THEY HAVE BEEN TRANSFORMED\
\
inertia = []\
for k in range(1, 11):\
    kmeans = KMeans(n_clusters=k, random_state=42)\
    kmeans.fit(scaled_data)\
    inertia.append(kmeans.inertia_)\
\
\
\
import matplotlib.pyplot as plt\
plt.plot(range(1, 11), inertia, marker='o')\
plt.xlabel('Number of Clusters')\
plt.ylabel('SSE(Inertia)')\
plt.title('Elbow Method')\
plt.show()\
\
optimal_cluster = 5\
kmeans = KMeans(n_clusters = optimal_cluster, random_state = 42)\
data['Cluster'] = kmeans.fit_predict(scaled_data)\
\
\
cluster_summary = data.groupby('Cluster').mean()\
print(cluster_summary)\
\pard\pardeftab720\partightenfactor0
\cf0 \
from sklearn.decomposition import PCA\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
pca = PCA(n_components = 2)\
\pard\pardeftab720\partightenfactor0
\cf0 reduced_data = pca.fit_transform(scaled_data)\
plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=data['Cluster'], cmap='viridis', s=50)\
plt.title('Cluster Visualization')\
plt.xlabel('PCA 1')\
plt.ylabel('PCA 2')\
plt.colorbar(label='Cluster')\
plt.show()\
}